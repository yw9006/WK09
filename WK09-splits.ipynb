{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Flow for Training/Fitting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_utils import PCA, RandomForestClassifier, StandardScaler\n",
    "from data_utils import classification_error, display_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Stages\n",
    "- Data Prep: Encoding, Scaling, PCA, sometimes Splitting into train/test datasets\n",
    "- Modeling: `fit()` classifier\n",
    "- Evaluation: predict and measure error\n",
    "\n",
    "#### Data Prep:\n",
    "Do we need to split our data, or is it already split into train/test sets?\n",
    "\n",
    "If it's already split we prepare the Encoding, Scaling, PCA objects using the `train` data (usually with the `fit_transform()` function), and then we use those same objects to encode, scale, PCA the `test` data (usually with the `transform()` function).\n",
    "\n",
    "If the data is not split into two datasets, we could first split it and repeat the steps above, or, although it might add a bit of bias to the models, we could perform Encoding, Scaling, PCA with `fit_transform()` on the entire dataset and then only split the already encoded, scaled, PCA'ed data. This biases the encoder, scaler, PCA models, and in turn, the model, but is a bit easier to perform.\n",
    "\n",
    "#### Modeling\n",
    "Once we have `train` and `test` datasets that has been encoded, scaled, PCA'ed, we can use the `train` dataset to fit a supervised model (classifier, regression, etc).\n",
    "\n",
    "Here we will usually call a `fit()` function with the training dataset's features and, separately, its labels or outcome variable values. Something like `fit(features, labels)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "We have a model we trained/fitted with the `train` dataset. Now we can measure how well it actually performs once it's used without the correct labels.\n",
    "\n",
    "Here we usually call `predict()` with a dataset's features to get label or regression predictions.\n",
    "\n",
    "We want to call `predict()` for both the `train` and `test` dataset, and then measure how close those predictions are to the actual labels and values that we have in our dataset.\n",
    "\n",
    "Eavluating with the `train` dataset will tell us if the model is capable of learning anything about the data. Evaluating with the `train` dataset will tell us if the model is capable of learning patterns and trends beyond the data that is fed to it.\n",
    "\n",
    "It's common for the model to perform better with the `train` data since it was trained using that data and labels, but the `test` dataset error is what's more important because it will tell us what kind of error to expect from data that the model hasn't seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Classifying irises based on measurements.\n",
    "\n",
    "Let's load a dataset and look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "display(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is an object with a bunch of keys, but if we look at the keys from the output above, we can see that the ones we want to look at are `data`, `target` and `feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't have separate train and test data. So, let's use this flow:\n",
    "\n",
    "<img src=\"./imgs/datasplit-00.jpg\" width=\"720px\"/>\n",
    "\n",
    "Let's create a dataframe from all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n",
    "iris_df[\"label\"] = iris[\"target\"]\n",
    "iris_df[\"flower type\"] = iris_df[\"label\"].apply(lambda x: iris[\"target_names\"][x])\n",
    "\n",
    "display(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $150$ samples of flowers, each of which has $4$ measurements, plus a label which indicates the type of flower.\n",
    "\n",
    "Although it might adversely affect our modeling a little bit, let's process this data before splitting it into $2$ datasets.\n",
    "\n",
    "### Prepare Data\n",
    "\n",
    "Let's scale the data using `StandardScaler` and run `PCA`. We need objects for both of those operations.\n",
    "\n",
    "the dataset has very few features, but let's use `PCA` to decrease it from $4$ to $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisScaler = StandardScaler()\n",
    "irisPCA = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the data using our scaler and PCA objects.\n",
    "\n",
    "We only do this to the features and not the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_scaled_df = irisScaler.fit_transform(iris_df.drop(columns=[\"label\", \"flower type\"]))\n",
    "iris_pca_df = irisPCA.fit_transform(iris_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at these results, just to make sure they make sense.\n",
    "\n",
    "After `StandardScaler` our features should have means of $0$ and standard deviations of $1$, and the datset should have $150$ rows and $4$ columns. The range of our features should all be between $-3$ and $3$ if they are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iris_scaled_df)\n",
    "display(iris_scaled_df.min(), iris_scaled_df.max(), iris_scaled_df.mean(), iris_scaled_df.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After PCA our dataset should have $150$ rows and only $2$ columns.\n",
    "\n",
    "The means and standard deviations here won't be standardized anymore, but that reflects the advantage of the `PCA` that gives more importance to features that carry more information about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iris_pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks ok.\n",
    "\n",
    "Before we train our model we have to split our data into `train` and `test` datasets.\n",
    "\n",
    "### Split the Data\n",
    "\n",
    "The features are coming from the `iris_pca_df` `DataFrame`, while the labels are coming from the original `iris_df` `DataFrame`. Since we didn't do any kind of re-ordering of the data, the order of the rows is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(iris_pca_df, iris_df[\"label\"], test_size=0.2, random_state=1010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should've split our dataset into $4$ `DataFrames`. The number of columns in the features `DataFrames` should be $2$ and the number of columns in the labels should be $1$.\n",
    "\n",
    "The number of rows for both of the test `DataFrame`s should be $30$ ($20\\%$ of $150$), while the length of the train `DataFrames` should be $120$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_features), len(train_labels))\n",
    "print(len(test_features), len(test_labels))\n",
    "print(len(train_features.columns), len(test_features.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model/Fit\n",
    "\n",
    "We can train our model now. We're going to use a `RandomForestClassifier` and `fit()` it with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisClassifier = RandomForestClassifier()\n",
    "\n",
    "irisClassifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "We can now run predictions for both `train` and `test` data and measure error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = irisClassifier.predict(train_features)\n",
    "test_predictions = irisClassifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before measuring the error we can check to see if these predictions have the right shapes and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_predictions), len(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "\n",
    "train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With previously-separated data\n",
    "\n",
    "Let's reload the dataset, put it into a `DataFrame` and split it before we do any of the pre-processing.\n",
    "\n",
    "So, following a flow like this now:\n",
    "\n",
    "<img src=\"./imgs/datasplit-01.jpg\" width=\"720px\"/>\n",
    "\n",
    "First, we'll put the `Python` object into a `DataFrame` like before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n",
    "iris_df[\"label\"] = iris[\"target\"]\n",
    "iris_df[\"flower type\"] = iris_df[\"label\"].apply(lambda x: iris[\"target_names\"][x])\n",
    "display(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "\n",
    "Now, before preprocessing the data, let's split it into $2$ datasets and check their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(iris_df, test_size=0.2, random_state=1010)\n",
    "display(train_df, test_df, len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Let's do our pre-processing (scaling, PCA). It's very similar, but now we have $2$ `DataFrames` to prepare.\n",
    "\n",
    "The `train` data is used to prepare/fit the scaling and PCA objects, and then the `test` data just has to be transformed by those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same objects\n",
    "irisScaler = StandardScaler()\n",
    "irisPCA = PCA(n_components=2)\n",
    "\n",
    "# fit and transform the training data, like before\n",
    "train_scaled_df = irisScaler.fit_transform(train_df.drop(columns=[\"label\", \"flower type\"]))\n",
    "train_pca_df = irisPCA.fit_transform(train_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the shape of the resulting `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_scaled_df)\n",
    "display(train_pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we process the `test` data. We already have prepared the scaler and PCA objects.\n",
    "\n",
    "We just have to use them to transform the `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_df = irisScaler.transform(test_df.drop(columns=[\"label\", \"flower type\"]))\n",
    "test_pca_df = irisPCA.transform(test_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_scaled_df)\n",
    "display(test_pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Model/Fit\n",
    "\n",
    "We have our `train` data ready, we can now use it to `fit()` a classifier model.\n",
    "\n",
    "We only `fit()` the `train` data, not the `test` data.\n",
    "\n",
    "We don't have separate label `DataFrames` like before, but we can easily grab them from the original `train` `DataFrame`, `train_df`.\n",
    "\n",
    "We can check if the length of those `DataFrames` are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_pca_df), len(train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisClassifier = RandomForestClassifier()\n",
    "\n",
    "irisClassifier.fit(train_pca_df, train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "We can now get predictions and measure error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = irisClassifier.predict(train_pca_df)\n",
    "test_predictions = irisClassifier.predict(test_pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if results have sensible shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_predictions), len(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure error, like before, but where we have to get the labels from the original `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = classification_error(train_df[\"label\"], train_predictions)\n",
    "test_error = classification_error(test_df[\"label\"], test_predictions)\n",
    "\n",
    "train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(train_df[\"label\"], train_predictions, display_labels=iris[\"target_names\"])\n",
    "display_confusion_matrix(test_df[\"label\"], test_predictions, display_labels=iris[\"target_names\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "9103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

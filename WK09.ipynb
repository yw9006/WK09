{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 09\n",
        "\n",
        "Unsupervised Learning: Distances, Clustering and PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/audio_utils.py\n",
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import Audio\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from audio_utils import fft, wav_to_list\n",
        "\n",
        "from data_utils import PCA, RandomForestClassifier, StandardScaler\n",
        "from data_utils import KMeansClustering, SpectralClustering\n",
        "from data_utils import object_from_json_url, classification_error, display_confusion_matrix\n",
        "\n",
        "from image_utils import make_image, open_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost and Distance Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The concept of **_distance_** is something that we saw and used in previous exercises but didn't talk too much about.\n",
        "\n",
        "**_Distance_** is how we tell how close two data points are to each other, and is the basis for clustering, classification and regression algorithms.\n",
        "\n",
        "In classification, we learn how to label new data based on how \"close\" it is to our already-labeled training data. In regression, we find parameters to equations that make our line-of-best-fit \"close\" to all of the points in the dataset. In clustering, we group data points in a way that minimizes distances between points within a cluster, while maximizing the distances between clusters. Distance is also an important concept for recommendation systems where we want to calculate when someone's taste is close to someone else's.\n",
        "\n",
        "### 1D\n",
        "\n",
        "The concept of distance in one dimension is pretty easy to understand: it's how far two points on a line are to each other. Physically, we can think of $1D$ distance as the distance between runners in a race, or, we can even think of time as a one-dimensional space where we measure distance between events in seconds, or minutes, or days.\n",
        "\n",
        "<img src=\"./imgs/dist1d.jpg\" height=\"220px\" />\n",
        "\n",
        "Each point in 1-dimensional space is described with a single variable, and the distance between any two points is just the absolute value of their difference:\n",
        "\n",
        "$\\displaystyle D(x_0, x_1) = |x_0 - x_1|$<br>\n",
        "$\\displaystyle D(x_0, x_2) = |x_0 - x_2|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2D\n",
        "\n",
        "Two-dimensional distances, where we have $2$ variables to describe each of our points, is also pretty familiar to us. This can be the distance between two cities on a map, measured in angles of longitude and latitude, or distances between two points in Manhattan, measured in streets and avenues.\n",
        "\n",
        "<img src=\"./imgs/dist2d.jpg\" width=\"95%\" />\n",
        "\n",
        "We have $2$ variables for each of our points and we also have $2$ ways in which we can combine them to measure distances in $2D$. The first is called $L1$, or Manhattan, distance, and it's the sum of the distances in each of the separate dimensions.\n",
        "\n",
        "$\\displaystyle D_{L1}(x_0y_0, x_1y_1) = |x_0 - x_1| + |y_0 - y_1|$\n",
        "\n",
        "The other way of measuring distances in $2D$ is using the $L2$, or Euclidean, distance formula:\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0, x_2y_2) = \\sqrt{(x_0 - x_2)^2 + (y_0 - y_2)^2}$\n",
        "\n",
        "This is pretty easy to understand as distances on a map, but... what if $x$ is a variable for $height$ in our dataset, and $y$ is the variable for $ear\\ length$? The calculations are still valid. As long as we remember to normalize our data, we can use the $L1$ or $L2$ formulas to figure out how \"close\" our data points are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3D\n",
        "\n",
        "Three-dimensional points have $3$ variables that describe them, and, while less common, it's still easy to understand how to measure the distance between them. We could be talking about the distance between planets, or between atoms, or between a wifi router and a cellphone. As long as the points aren't on a plane, we need $3$ variables to describe them and measure the distance between them.\n",
        "\n",
        "<img src=\"./imgs/dist3d.jpg\" width=\"95%\" />\n",
        "\n",
        "We can extend the $L1$ and $L2$ distance formulas to work in $3D$:\n",
        "\n",
        "$\\displaystyle D_{L1}(x_0y_0z_0, x_1y_1z_1) = |x_0 - x_1| + |y_0 - y_1| + |z_0 - z_1|$\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0z_0, x_2y_2z_2) = \\sqrt{(x_0 - x_2)^2 + (y_0 - y_2)^2 + (z_0 - z_2)^2}$\n",
        "\n",
        "This works even when our $3$ variables aren't actually physical locations. If $x$ is a variable that keeps track of the number of rooms in a house, $y$ a variable for the age of the house, and $z$ the total area of the house, we can use the above formulas to measure how \"close\" two houses in our dataset are (after we normalize our data, of course).\n",
        "\n",
        "### N-Dimensions\n",
        "\n",
        "Most of the datasets we've seen so far already have more then $3$ features/dimensions... what then?\n",
        "\n",
        "Well... the $L1$ and $L2$ distance formulas can be used regardless of the number of features/dimensions in our dataset. We just keep adding parameters to our formula:\n",
        "\n",
        "$\\displaystyle D_{L1} = \\sum_{d}{|A_d - B_d|}$ (for all dimensions $d$)\n",
        "\n",
        "$\\displaystyle D_{L2} = \\sqrt{\\sum_{d}{(A_d - B_d)^2}}$ (for all dimensions $d$)\n",
        "\n",
        "So even when we have a dataset with $15$ or $20$ features/dimensions, we can still get some idea of how \"close\" two points in that dataset are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other Distance Formulas\n",
        "\n",
        "$L1$ and $L2$ are definitely the most widely used distance formulas in ML applications, but they aren't the only ones. Two other types of distances are:\n",
        "\n",
        "#### Cosine Similarity\n",
        "\n",
        "When we're dealing with datasets that are very sparse (there are more dimensions than points), and the $L1$ and $L2$ distances that separate the data points are really huge, we might want to measure the cosine similarity between two points instead.\n",
        "\n",
        "<img src=\"./imgs/distcos.jpg\" height=\"300px\" />\n",
        "\n",
        "In the drawing above, instead of measuring the direct distances between $x_0y_0$, $x_1y_1$ and $x_2y_2$, we can pick a separate reference point and measure the cosine of the angles formed by lines drawn from the reference point to each of the other points. We can see that angle $\\theta_{12}$ is smaller than $\\theta_{01}$ and $\\theta_{02}$, which means that $(x_1y_1, x_2y_2)$ is the pair of most similar points.\n",
        "\n",
        "Points with cosine values close to $1$ are in the same direction in space; points with cosine values close to $0$ are in perpendicular directions, and points with cosine values close to $-1$ are in opposite directions.\n",
        "\n",
        "$\\displaystyle cos(A, B) = \\frac{A \\cdot B}{ \\left|\\left|A\\right|\\right| \\left|\\left|B\\right|\\right|}$\n",
        "\n",
        "$\\displaystyle cos(x_0y_0, x_1y_1) = \\frac{x_0x_1 + y_0y_1}{\\sqrt{x_0^2+y_0^2} \\sqrt{x_1^2+y_1^2}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mahalanobis Distance\n",
        "\n",
        "This is useful for measuring distances between points that are part of a collection of points.\n",
        "\n",
        "<img src=\"./imgs/distmana-01.jpg\" height=\"250px\" />\n",
        "\n",
        "In the drawing above, if we only had the points $x_0y_0$, $x_1y_1$ and $x_2y_2$, we could use $L2$ distances and everything is fine:\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0, x_1y_1) > D_{L2}(x_0y_0, x_2y_2)$.\n",
        "\n",
        "But, if instead, $x_0y_0$, $x_1y_1$ and $x_2y_2$ are part of a collection of points with a well-defined average and standard deviation, like the image below, we should use something that makes more common distances shorter, and rarer distances larger.\n",
        "\n",
        "<img src=\"./imgs/distmana-02.jpg\" height=\"250px\" />\n",
        "\n",
        "In this case, where $x_0y_0$ and $x_2y_2$ are on the extremes of the distribution, we want the distance between them to be larger than the distance between $x_0y_0$ and $x_1y_1$, which happens along a more common direction of the data.\n",
        "\n",
        "In order to have $\\displaystyle D_{M}(x_0y_0, x_2y_2) > D_{M}(x_0y_0, x_1y_1)$, we have to take into account the distribution of our data: its mean, standard deviation and covariances. One way to do that is using the formula below:\n",
        "\n",
        "$\\displaystyle D_{M}(A, B) = \\sqrt{(A - B)^2 V_I}$.\n",
        "\n",
        "Where $V_I$ is the inverse of the covariance matrix of our points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "### Clustering\n",
        "\n",
        "#### More Wine ! 🍷🍷🍷\n",
        "\n",
        "Let's pretend we own an online wine store.\n",
        "\n",
        "Last week we created a model that predicts wine quality based on a bunch of its properties. We could use this model to figure out how much to pay suppliers for the wine, and how much to charge costumers.\n",
        "\n",
        "But, maybe this \"`quality`\" feature might not be something we want to share with our costumers. Even though it's based on data, it sounds abstract and subjective and would require explanations about our data and our process, which could create confusion.\n",
        "\n",
        "Using all six features from the original dataset (`alcohol`, `acidity`, `density`, etc) might also not be very useful for costumers who want to buy new wines that are similar to ones that they have previously liked.\n",
        "\n",
        "What we can do instead is classify the wines into groups that take into account all of the features of the dataset, but present costumers with a more manageable amount of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recommendations\n",
        "\n",
        "What we're really hoping to have is a simple recommendation system for our costumers, where we can recommend wines based on previous wines they liked, without them having to know the $6$ features of the previous wines.\n",
        "\n",
        "There are a few ways of doing this, but the strategy we'll take is called clustering.\n",
        "\n",
        "### Clustering\n",
        "\n",
        "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis), or cluster analysis, is an example of an *unsupervised* learning method that groups items based on their many features and properties.\n",
        "\n",
        "We'll use it to divide our wines in such a way that wines in the same group, or *cluster*, are more similar to each other than to wines in other clusters.\n",
        "\n",
        "These clusters won't necessarily correlate directly to the features in our dataset, but will be computed using a combination of the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Learning\n",
        "\n",
        "The models that we've trained so far for doing regression and classification are considered *supervised* models. During training we give the model our input features, but also provide it with the *correct* values for the output signals. These output signals tend to be human-labeled values, and are sometimes called the *supervisory signals*.\n",
        "\n",
        "When fully-labeled training data is processed during training, we are hoping that the model learns to extrapolate what it *sees* in the labeled data to new, unseen, unlabeled instances of data with the same input features, but unknown output values.\n",
        "\n",
        "#### Supervised Classification:\n",
        "\n",
        "Given a set of initial data points with labels:<br>\n",
        "<img src=\"./imgs/classification-02.jpg\" width=\"620px\"/>\n",
        "\n",
        "We create a model that learns to assign labels to the original points:<br>\n",
        "<img src=\"./imgs/classification-03.jpg\" width=\"620px\"/>\n",
        "\n",
        "so that later we can assign correct labels to new data points:<br>\n",
        "<img src=\"./imgs/classification-04.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsupervised Learning\n",
        "\n",
        "Unlike supervised learning, unsupervised models learn patterns from unlabeled data. This means all of the features are considered input features, and there are no separate output features or signals. The idea is that by analyzing and processing data in specific ways, the model is able to build a concise representation of its features and create new ways of interpreting, visualizing or generating similar data.\n",
        "\n",
        "We can use unsupervised learning models to explore new datasets and try to simplify our data before we do any kind of supervised learning.\n",
        "\n",
        "We can also use supervised learning to build recommendation systems that learn how to group items by their many features or characteristics.\n",
        "\n",
        "The steps for training an unsupervised model should seem familiar:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Select variables and features to be considered\n",
        "5. Create a model\n",
        "6. Run model on input data and test data\n",
        "7. Measure error\n",
        "\n",
        "Even though it all looks familiar, that last step isn't very obvious.\n",
        "\n",
        "How do we measure error on a model that doesn't have a set of correct answers?\n",
        "\n",
        "Maybe *error* is not the right term, but we'll see how to define *metrics* to score and measure our unsupervised models.\n",
        "\n",
        "#### Unsupervised Clusterings:\n",
        "Since there are no correct labels, both of the following clusterings are valid!\n",
        "\n",
        "<img src=\"./imgs/clustering-00.jpg\" width=\"620px\"/>\n",
        "\n",
        "<img src=\"./imgs/clustering-01.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run it !\n",
        "\n",
        "### Preparing Data\n",
        "\n",
        "We'll load the same wine dataset as last week and normalize its features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wine_scaler = StandardScaler()\n",
        "wines_scaled = wine_scaler.fit_transform(wines_df)\n",
        "\n",
        "## 4. Select variables to be considered\n",
        "##    We're gonna drop the quality features to avoid re-clustering by quality\n",
        "features = wines_scaled.drop(columns=[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusterings\n",
        "\n",
        "Let's look at our first clustering algorithm:\n",
        "\n",
        "#### [K-means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means):\n",
        "Tries to separate the data into $k$ groups with similar properties. Requires the number of clusters to be determined beforehand, and the algorithm tries to minimize the difference between objects in a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters = 4\n",
        "\n",
        "## 5. Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## 6. Run the model on the data\n",
        "km_predicted = km_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plots\n",
        "\n",
        "Let's pick $2$ or $3$ variables to visualize our data and clusters.\n",
        "\n",
        "This could be any of our features, but let's look at the *covariances* table and pick features related to the highest covariance magnitudes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Look at covariances of features\n",
        "features.cov()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The highest pairs are `alcohol`/`density` and `alcohol`/`chlorides`\n",
        "\n",
        "Let's plot `alcohol`, `chlorides` and `density` then."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For plotting\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "clusters = km_predicted[\"clusters\"]\n",
        "\n",
        "# 2D\n",
        "plt.scatter(x, y, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"k-means clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(yl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"k-means clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "ax.set_title(\"k-means clustering\")\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Number of clusters\n",
        "\n",
        "Does the number of clusters affect clustering ?\n",
        "\n",
        "Change the variable and re-run clustering to see how the groupings change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: change n_clusters above and re-run clustering\n",
        "\n",
        "n_clusters = 3\n",
        "\n",
        "## 5. Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## 6. Run the model on the data\n",
        "km_predicted = km_model.fit_predict(features)\n",
        "clusters = km_predicted[\"clusters\"]\n",
        "\n",
        "\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "# 2D\n",
        "plt.scatter(x, y, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"k-means clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(yl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"k-means clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "ax.set_title(\"k-means clustering\")\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "What changes ? Does one choice seem better than the others ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other Clustering Options\n",
        "\n",
        "There are many algorithms for clustering data, that differ in the assumptions they make about the data, the parameters that should be optimized, and the amount of preprocessing that is performed on the data.\n",
        "\n",
        "There's one method called [Gaussian Clustering](https://scikit-learn.org/stable/modules/mixture.html#mixture) that is similar to K-means, but it assumes that all features of our data can be modeled as [Gaussian distributions](https://en.wikipedia.org/wiki/Normal_distribution). During clustering, the algorithm tries to minimize the standard deviation of the distribution of each cluster.\n",
        "\n",
        "Another clustering method, [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering), automatically combines and removes a few of our features, before doing K-means clustering. This should always be as good as, or better than, regular K-means clustering in terms of minimizing the distances between each point and its cluster's center.\n",
        "\n",
        "[Scikit-Learn](https://scikit-learn.org/) has implementations for these two clustering algorithms, and many more. They are called `GaussianClustering` and `SpectralClustering`, and their constructors take the same parameters as the `KMeansClustering` constructor.\n",
        "\n",
        "Repeat steps $5$ and $6$ to create a model using `SpectralClustering` and run it on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters = 4\n",
        "## 5. Create Clustering object\n",
        "sc_model = SpectralClustering(n_clusters=n_clusters)\n",
        "\n",
        "# TODO: Run the model\n",
        "## 6. Run the model on the data\n",
        "sc_predicted = sc_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot Spectral Clustering results\n",
        "\n",
        "clusters = sc_predicted[\"clusters\"]\n",
        "\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "# 2D\n",
        "plt.scatter(x, y, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"spectral clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(yl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"spectral clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "ax.set_title(\"spectral clustering\")\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Are the results any different than k-mean clustering ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring\n",
        "\n",
        "Would be nice to have a way to measure how good these clusters actually are.\n",
        "\n",
        "It would help determine if we need more clusters, or if one method is actually better than the other.\n",
        "\n",
        "There are a couple of ways to do this. We'll look at three of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distance\n",
        "\n",
        "The first kind of scoring uses the sum of the distances between each point and its cluster's center as a metric.\n",
        "\n",
        "Each cluster's center is represented by the average values of all of the features of all of its members: $(\\overline{F_0}, \\overline{F_1}, \\overline{F_2}, ...)$. Once we know that we can use the L2-distance we saw above to calculate and accumulate the distances from each point to its cluster's center.\n",
        "\n",
        "A smaller cluster distance means that the cluster center is a good representation of its members.\n",
        "\n",
        "Luckily, our clustering models have a `distance_error()` function that can be used to report the distance error, after `fit()` has been called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KMeans distance error:\", km_model.distance_error())\n",
        "print(\"Spectral distance error:\", sc_model.distance_error())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Likelihood\n",
        "\n",
        "The second way of scoring clusters treats each cluster as a potential normal distribution, and then calculates the likelihood that each point came from its cluster distribution.\n",
        "\n",
        "Values closer to zero mean that the clusters' statistical properties (mean, variation) are good estimators for the data.\n",
        "\n",
        "Our model objects also have a `likelihood_error()` function we can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KMeans likelihood error:\", km_model.likelihood_error())\n",
        "print(\"Spectral likelihood error:\", sc_model.likelihood_error())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although somewhat related, the `distance` and `likelihood` calculations measure different things, and are in different units.\n",
        "\n",
        "We can't compare distances to likelihoods to draw any kind of conclusion.\n",
        "\n",
        "What we want to do is use either one of these metrics to select a clustering method and tune its parameters.\n",
        "\n",
        "### Balance\n",
        "\n",
        "A final metric we can consider when analyzing different clustering algorithms and strategies is to see how balanced the resulting clusters are. This isn't always important; we might have categories of items or events that are more common than others, and will produce unequal cluster groups.\n",
        "\n",
        "In other cases, where we know we want to have groups of similar sizes, this is a good metric to look at. For example, if we were to use the body measurement dataset for deciding how many sizes of bike helmets to produce, we should probably have sizes that cover similar portions of the population, and avoid very bespoke sizes that only fit few people.\n",
        "\n",
        "We compute `balance error` by summing the differences between our cluster sizes and the sizes of a perfectly balanced clustering. Once we have this sum, we scale it to get a number between $0$, for a perfectly balanced clustering, and $1$, for a most-unbalanced clustering.\n",
        "\n",
        "$\\displaystyle balance\\ error = \\frac{1}{2} \\left(\\frac{n}{n-1}\\right) \\sum_{i=1}^{n}{\\left|\\frac{C_i}{C_0 + C_1 + ... + C_n} - \\frac{1}{n}\\right|}$\n",
        "\n",
        "The $\\frac{C_i}{C_0 + C_1 + ... + C_n}$ terms are the sizes of our $n$ clusters expressed as the percentage of the total number of items in all clusters. The $\\frac{1}{n}$ term is the size of each cluster in a perfectly balanced clustering. We sum up these differences and scale it all by $\\frac{1}{2} \\left(\\frac{n}{n-1}\\right)$ to get a number between $0$ and $1$.\n",
        "\n",
        "We don't have to focus too much on this math right now. It's here for completeness and because it's good to practice reading an algorithm described as text, math equations and code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Balance Error\n",
        "\n",
        "Luckily this has also been implemented for us and we can get our model's `balance error` by calling the `balance_error()` function of our clustering object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KMeans balance error:\", km_model.balance_error())\n",
        "print(\"Spectral balance error:\", sc_model.balance_error())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of clusters\n",
        "\n",
        "If we consider the distance and balance metrics for the $2$ algorithms, it seems like `SpectralClustering` has smaller distance error, but doesn't produce the most balanced clusters.\n",
        "\n",
        "Let's try different cluster numbers for each algorithm to see if there's a *better* way of clustering our wines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try 2 - 10 clusters for K-Means Clustering\n",
        "num_clusters = list(range(2,10))\n",
        "\n",
        "# collect distance, likelihood and balance errors\n",
        "dist_err = []\n",
        "like_err = []\n",
        "bala_err = []\n",
        "\n",
        "# get distance, likelihood and balance for different clustering sizes\n",
        "for n in num_clusters:\n",
        "  mm = KMeansClustering(n_clusters=n)\n",
        "  mm.fit_predict(features)\n",
        "  dist_err.append(mm.distance_error())\n",
        "  bala_err.append(mm.balance_error())\n",
        "\n",
        "# plot errors as function of number of clusters\n",
        "plt.plot(num_clusters, dist_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distance Error\")\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.ylim([0, 2.5])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, bala_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Balance Error\")\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.ylim([0, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try 2 - 10 clusters for Spectral Clustering\n",
        "num_clusters = list(range(2,10))\n",
        "\n",
        "# collect distance, likelihood and balance errors\n",
        "dist_err = []\n",
        "like_err = []\n",
        "bala_err = []\n",
        "\n",
        "# get distance, likelihood and balance for different clustering sizes\n",
        "for n in num_clusters:\n",
        "  mm = SpectralClustering(n_clusters=n)\n",
        "  mm.fit_predict(features)\n",
        "  dist_err.append(mm.distance_error())\n",
        "  bala_err.append(mm.balance_error())\n",
        "\n",
        "# plot errors as function of number of clusters\n",
        "plt.plot(num_clusters, dist_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distance Error\")\n",
        "plt.title(\"Spectral Clustering\")\n",
        "plt.ylim([0, 2.5])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, bala_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Balance Error\")\n",
        "plt.title(\"Spectral Clustering\")\n",
        "plt.ylim([0, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Doesn't seem like the number of clusters affects the distance error for `KMeansClustering` too much.\n",
        "\n",
        "Based on balance error it seems like $5$ could be a good number of clusters for this model, based on the plot of distance error for both the `KMeansClustering` and `SpectralClustering` methods.\n",
        "\n",
        "Let's re-cluster and look at how our data gets clustered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 5\n",
        "m_model = KMeansClustering(n_clusters=n)\n",
        "km_predicted = m_model.fit_predict(features)\n",
        "\n",
        "clusters = km_predicted[\"clusters\"]\n",
        "\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "# 2D\n",
        "plt.scatter(x, y, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(yl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 5\n",
        "m_model = SpectralClustering(n_clusters=n)\n",
        "sc_predicted = m_model.fit_predict(features)\n",
        "\n",
        "clusters = sc_predicted[\"clusters\"]\n",
        "\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "# 2D\n",
        "plt.scatter(x, y, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"Spectral Clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(yl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"Spectral Clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.ylim([-2.2, 3])\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "plt.title(\"Spectral Clustering\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis\n",
        "\n",
        "So, even though $5$ clusters gave us some of the smallest error values for both clustering methods, some of the clusters are really small and hard to find on the graphs.\n",
        "\n",
        "And the cluster sizes are really unequal.\n",
        "\n",
        "If this clustering is to be used for recommending wines to costumers, maybe using  $3$ or $4$ clusters is a more sensible way of grouping our wines. Not because we have to balance the cluster sizes, but because the subtleties of having $5$ categories of wine might be less easy to explain.\n",
        "\n",
        "Using $3$ or $4$ categories is probably more legible. The categories could be something like: `strong` for the more alcoholic wines, `bold` and/or `dense` for the ones that are less alcoholic, but have high density, and `wild` for the ones high in chlorides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Images\n",
        "\n",
        "Clustering can also be used to help analyze image and audio files.\n",
        "\n",
        "We can extract color information from an image by clustering its pixels by their RGB values. This can be used to get an estimate of the most \"important\" colors in an image. They're not the most common colors, necessarily, but the colors necessary to represent the image.\n",
        "\n",
        "This is called `color quantization` and is a kind of compression because we reduce the total number of colors in an image from a possible $16\\text{,}581\\text{,}375$ unique colors to $4$, $8$, $16$, etc... colors while preserving the overall appearance of the image. The calculated cluster centers become the color palette of the image, and we can re-color the image using only those colors.\n",
        "\n",
        "We start by loading an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mimg = open_image(\"./data/imgs/arara.jpg\")\n",
        "\n",
        "display(mimg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colors as features\n",
        "\n",
        "Since this process works on the pixels of a single image, we can think of each pixel as a measurement and its `R`, `G` and `B` values as its features.\n",
        "\n",
        "Before we put our image through clustering we should turn it into a `DataFrame` where the rows are the pixels and the columns are the `R`, `G`, `B` values.\n",
        "\n",
        "We can use the `pd.DataFrame.from_records()` function, but first we have to turn our pixel list into a list of objects:\n",
        "\n",
        "`[ [R,G,B], [R,G,B], [R,G,B], ... ]` -> `[{\"R\": R, \"G\": G, \"B\": B}, {\"R\": R, \"G\": G, \"B\": B}, ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: create list of objects from list of lists\n",
        "pixelObj = [{\"R\":r, \"G\":g, \"B\":b} for r,g,b in mimg.pixels]\n",
        "\n",
        "img_df = pd.DataFrame.from_records(pixelObj)\n",
        "\n",
        "img_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding/Scaling\n",
        "\n",
        "Not needed! All our features are already numbers, and they're all in the same units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot\n",
        "\n",
        "We just opened a new \"dataset\"... let's plot it.\n",
        "\n",
        "Since our features are the `R`, `G` and `B` channel values for each pixel, we can actually plot these in $3D$.\n",
        "\n",
        "Each of the features will place a point in `x`, `y`, `z` space, and the color of the point can be the color of the pixel.\n",
        "\n",
        "The only funny business here is that `pyplot` expects colors in the range $[0, 1]$ and not $[0, 255]$, so we have to convert those.\n",
        "\n",
        "We could use a `MinMaxScaler`, but since the inputs are already in a known range and the math is easy, we'll just use a comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert [0, 255] to [0, 1]\n",
        "c = [(r/255, g/255, b/255) for r,g,b in mimg.pixels]\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(img_df[\"R\"], img_df[\"G\"], img_df[\"B\"], c=c, marker='o', linestyle='', alpha=0.5)\n",
        "ax.set_xlabel(\"R\")\n",
        "ax.set_ylabel(\"G\")\n",
        "ax.set_zlabel(\"B\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster !\n",
        "\n",
        "Set up the `Clustering` object and run `fit_predtict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters=8\n",
        "\n",
        "## Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## Run the model on the pixel data\n",
        "km_predicted = km_model.fit_predict(img_df)\n",
        "\n",
        "km_predicted.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Un-Cluster / Group\n",
        "\n",
        "The `km_predicted` variable holds a `DataFrame` that is a mapping from pixel index to cluster index, but what we really want is to re-map our original pixels into the color palette made up of our cluster centers.\n",
        "\n",
        "The `KMeansClustering` object has a member variable called `cluster_centers_` that holds the centers of our clusters. We can use this to build a new pixel array for our image.\n",
        "\n",
        "Let's take a look at the cluster centers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have $4$ lists inside the `cluster_centers_` list. They should represent colors, but they're using floating point numbers right now, which will most likely give us troubles when we try to turn these into an image.\n",
        "\n",
        "Let's transform these into `int`s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: use round() or int() to ensure our cluster centers are valid color values (ints)\n",
        "color_centers = [[round(r), round(g), round(b)] for r,g,b in km_model.cluster_centers_]\n",
        "\n",
        "print(color_centers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use these in a pixel array.\n",
        "\n",
        "Let's iterate through the `km_predicted[\"clusters\"]` and use those values to push their corresponding cluster center colors into a new pixel array.\n",
        "\n",
        "We go through `km_predicted[\"clusters\"]`, and if we see cluster $0$ we want to push `color_centers[0]` onto our pixel array, etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clustered_pxs = []\n",
        "\n",
        "# TODO: iterate through the cluster list and append the right color for each pixel\n",
        "for gidx in km_predicted[\"clusters\"]:\n",
        "  clustered_pxs.append(color_centers[gidx])\n",
        "\n",
        "display(make_image(clustered_pxs, mimg.size[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio (Optional)\n",
        "\n",
        "We can also use clustering to help analyze audio files after we've run an `FFT` analysis.\n",
        "\n",
        "**Review**: The `FFT` analysis tell us which audio frequencies are present in our file. The function returns $2$ lists, one of frequencies present, and another of how much energy is present in each of those frequencies. Higher energy for a frequency means that frequency is more dominant in the sound.\n",
        "\n",
        "We would like to use the `FFT` as a way to extract a reduced, but meaningful, set of features from our audio files to use in some kind of classification model.\n",
        "\n",
        "Let's open up an audio, plot it, play it, run `FFT` and look at the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open up audio file\n",
        "sound_file_path = \"./data/sounds/piano-8.wav\"\n",
        "my_samples = wav_to_list(sound_file_path)\n",
        "\n",
        "# Plot it\n",
        "plt.plot(my_samples)\n",
        "plt.show()\n",
        "\n",
        "# Play it\n",
        "display(Audio(sound_file_path))\n",
        "\n",
        "# FFT it\n",
        "fft_energy, fft_freqs = fft(my_samples)\n",
        "\n",
        "print(\"num samples:\", len(my_samples))\n",
        "print(\"num frequencies:\", len(fft_freqs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🤔\n",
        "\n",
        "For an audio file with $80\\text{,}000$ samples, our `FFT` returns two lists of $40\\text{,}000$ values, so not really a reduction in features.\n",
        "\n",
        "### Plot FFT\n",
        "Let's plot the result of the `fft()` command and see how to bring this number down to something more legible and meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot FFT\n",
        "plt.plot(fft_freqs, fft_energy, \"o\", markersize=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🤔\n",
        "\n",
        "Only a portion of the frequencies seem to have any kind of signal.\n",
        "\n",
        "If we zoom into the parts with energy, we can see that we have some dominant frequencies, but determining what they are automatically can be kind of messy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot FFT\n",
        "plt.plot(fft_freqs, fft_energy, \"o\", markersize=2)\n",
        "plt.xlim([0, 15000])\n",
        "plt.ylim([0, 100000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter\n",
        "\n",
        "Let's filter out the frequencies with low energy and round the frequency values to whole numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fft_energy_f = []\n",
        "fft_freqs_f = []\n",
        "\n",
        "E_THRESHOLD = 2e3\n",
        "\n",
        "for e, f in zip(fft_energy, fft_freqs):\n",
        "  if e > E_THRESHOLD:\n",
        "    fft_energy_f.append(e)\n",
        "    fft_freqs_f.append(round(f))\n",
        "\n",
        "# Re-plot FFT\n",
        "plt.plot(fft_freqs_f, fft_energy_f, \"o\", markersize=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster Prep\n",
        "\n",
        "This looks better and we can try to cluster these by frequency to get a better understanding of the dominant frequencies of the audio.\n",
        "\n",
        "Let's first put the frequency and energy into a `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fft_f_df = pd.DataFrame({ \"frequency\": fft_freqs_f, \"energy\": fft_energy_f })\n",
        "\n",
        "display(fft_f_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster by Frequency\n",
        "\n",
        "Let's find clusters for the filtered frequency values.\n",
        "\n",
        "The number of clusters is something that can be adjusted if we feel the need. It will determine how many features we end up with in our transformed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters=16\n",
        "\n",
        "# Create Clustering object\n",
        "km_freq_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "# Run the model on the fft frequency data\n",
        "km_freq_predicted = km_freq_model.fit_predict(fft_f_df[[\"frequency\"]])\n",
        "\n",
        "print(km_freq_model.cluster_centers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Clusters\n",
        "\n",
        "Plot the filtered frequencies and the cluster centers on the same graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot FFT and cluster centers\n",
        "plt.plot(fft_f_df[[\"frequency\"]], fft_f_df[[\"energy\"]], \"o\", markersize=2)\n",
        "for f in km_freq_model.cluster_centers_:\n",
        "  plt.scatter(f[0], f[-1], c='red', s=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Refine\n",
        "\n",
        "We have reduced the number of features of our audio file from $80\\text{k}$ to $16$, which is great, but some of those $16$ frequencies might not actually be very representative of the audio because they represent frequencies with pretty low energies.\n",
        "\n",
        "What if we included both `frequency` and `energy` in our clustering?\n",
        "\n",
        "We don't even have to filter the original lists from the `fft()` command because we could do filtering later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FFT of our samples\n",
        "fft_energy, fft_freqs = fft(my_samples)\n",
        "\n",
        "# put raw fft results in DataFrame this time\n",
        "fft_df = pd.DataFrame({\"frequency\": fft_freqs, \"energy\": fft_energy})\n",
        "\n",
        "# TODO: repeat clustering, but using both columns of our DataFrame\n",
        "\n",
        "# Create Clustering object\n",
        "km_freq_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "# Run the model on the fft data\n",
        "km_freq_predicted = km_freq_model.fit_predict(fft_f_df)\n",
        "\n",
        "# print cluster centers\n",
        "print(km_freq_model.cluster_centers_)\n",
        "\n",
        "# Plot FFT and cluster centers\n",
        "# Plot FFT and cluster centers\n",
        "plt.plot(fft_f_df[[\"frequency\"]], fft_f_df[[\"energy\"]], \"o\", markersize=2)\n",
        "for f in km_freq_model.cluster_centers_:\n",
        "  plt.scatter(f[0], f[-1], c='red', s=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Some of the clusters are still centered on frequencies with low energy, but because our cluster centers now have $2$ values we can tell how much energy each of these cluster centers represent and we can sort the cluster centers by their energy to get a final feature set.\n",
        "\n",
        "Even if we don't do this sorting, we now have $16$ clusters, each with $2$ values, so a total of $32$ features that could be used to classify each audio file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Unsupervised Learning\n",
        "\n",
        "### Dimensionality Reduction\n",
        "\n",
        "A few homeworks ago we looked at an image classification exercise and we saw that creating image filters by hand and selecting which features to look at can be a very laborious and complex process. Let's now combine what we learned about data normalization, distances, and learning algorithms to try to create a better image classification system.\n",
        "\n",
        "Consider the following image that is $4$ pixels wide by $4$ pixels high, it has a total of $16$ pixels:\n",
        "\n",
        "<img src=\"./imgs/pixdim-00.jpg\" height=\"200px\" />\n",
        "\n",
        "We can think of these $16$ pixels as the features of the image, and if we had a dataset of images of this size, we can think of the rows in this dataset as our different images and then the columns being the $16$ values for each of the pixels of those images.\n",
        "\n",
        "And the same is true for a dataset that has images that are $256$ by $256$ pixels:\n",
        "\n",
        "<img src=\"./imgs/pixdim-01.jpg\" height=\"200px\" />\n",
        "\n",
        "We would just have as many rows as we have images, and each of those rows would have $65,536$ columns/features, one for each of its pixels.\n",
        "\n",
        "We might be tempted now to develop some kind of method that uses $L2$ distances or cosine similarities to classify images by finding similarities between their pixel content. And that might work sometimes, but consider that computing the $L2$ distance between $2$ of our $256$ x $256$ pixel images would require squaring over $65000$ terms... and if we look at the black and white $4$ x $4$ images below, it's not so clear that subtracting raw pixel values would give us any indication of similarity:\n",
        "\n",
        "<img src=\"./imgs/pixdiff.jpg\" height=\"200px\" />\n",
        "\n",
        "The difference between the first image and the second image is of $8$ pixels. The difference between the first and third image is also of $8$ pixels, while the difference between the second and third image is of $16$ pixels, even though they're more similar. The same could be calculated for the last two pictures and the first.\n",
        "\n",
        "What we want to do instead is something called **_dimensionality reduction_**, where we turn those $65,000$ columns of pixel values into a smaller number of columns that represent more meaningful and complex information about the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Projection\n",
        "\n",
        "One way of reducing the dimensions of our dataset is by doing something called **_feature projection_**, which is very simple to perform and understand. We did this when we dropped features from the wine dataset last week.\n",
        "\n",
        "**_Feature Projection_** is when we just drop the columns in our dataset that we think are less important. The results of doing this can be seen in the following two images:\n",
        "\n",
        "<img src=\"./imgs/dimredproj-00.jpg\" height=\"350px\" />\n",
        "\n",
        "On the left side we have a graph of our data set in two dimensions. On the right, at the very bottom, we've projected all of those points straight down onto the x-axis by just ignoring their $y$ components.\n",
        "\n",
        "On next image we can see what the problem with doing that might be. Points that are actually further apart in our full dataset will seem closer once projected to a lower dimension like this, and points that are close together in $2D$ space can seem further apart in $1D$ space.\n",
        "\n",
        "<img src=\"./imgs/dimredproj-03.jpg\" height=\"350px\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Space-Filling Curves\n",
        "\n",
        "Another way of reducing dimensions is by using **_space-filling curves_**, like the [Hilbert curve](https://en.wikipedia.org/wiki/Hilbert_curve) or [Z-order curves](https://en.wikipedia.org/wiki/Z-order_curve).\n",
        "\n",
        "These functions reorder our data by tracing a curvy path between all the points in our dataset. If we look at the following images:\n",
        "\n",
        "<img src=\"./imgs/dimredcurve-00.jpg\" height=\"350px\" />\n",
        "\n",
        "The image on the left again shows our $2D$ data points graphed on a plane. On the right, we've drawn a line that connects all of our points by starting with a point on the lower-left side of the graph and then connecting to the nearest point, and then the next nearest point and so on and so on. In the end we have the light-blue line that connects and re-orders all of our $2D$ points into a $1D$ line.\n",
        "\n",
        "The problem with this method is that **_space-filling curves_** still have some discontinuities, or jumps, which might result in the same problem as **_projection_**, where closer points become further apart after transformation and vice-versa.\n",
        "\n",
        "### Principal Component Analysis\n",
        "\n",
        "One final way of reducing the dimensions in our dataset is by doing something called **_principal component analysis_** or **_PCA_**. This is a kind of unsupervised learning algorithm that looks at all of our original data and tries to figure out the directions in which it spreads out the most.\n",
        "\n",
        "<img src=\"./imgs/dimredpca-00.jpg\" height=\"350px\" />\n",
        "\n",
        "Again, on the left we have the original $2D$ data on a plane. On the right we see the original data with a line that represents the direction in which the data seems to spread.\n",
        "\n",
        "**_PCA_** is kind of, sort of, like doing **_Linear Regression_** in a special manner, and finding many lines-of-best-fit, but with special properties. We can think of these new lines as new axes that we re-orient our data around, and instead of having the $x$ axis and the $y$ axis we'll have $2$ new axes, one in the direction that captures the most variation in our data, and another in a perpendicular direction, that captures less of the variation in our data, like below:\n",
        "\n",
        "<img src=\"./imgs/dimredpca-03.jpg\" height=\"350px\" />\n",
        "\n",
        "But, wait ! If we have $N$ dimensions to begin with and we end up with these $N$ lines/axis/features, isn't it the same as before ? We still have $N$ dimensions to deal with!\n",
        "\n",
        "Yes, but the thing about the **_PCA_** axes is that these lines, or, principal components, are actually ordered by importance, so the very first principal component is the one that captures most of the variation in our data, the second one captures the second most amount of variation in our data, the third one the third most, and so on and so on...\n",
        "\n",
        "What we want to do after we compute the principal components of our dataset is perform a **_projection_** and chop off most of the new dimensions. Most of the information about our data will be captured by the first couple of principal components, so we can now project our data onto a few of these new axes and they'll retain most of the information about our data.\n",
        "\n",
        "<img src=\"./imgs/dimredpca-02.jpg\" height=\"350px\" />\n",
        "\n",
        "In the $2D$ case above if we only consider the first principal component and project our data onto that blue line we retain most of the information about the relative distances between the points in our dataset, but since they're all projected onto a line, they can be represented by just $1$ feature/value. In the end we were able to reduce our data from having $2$ variables in the $XY$ plane to just having one variable in the direction of the first principal component.\n",
        "\n",
        "Let's look at some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA in 3D\n",
        "\n",
        "Let's reload the `ANSUR` data and look only at the features about hands. We'll use PCA to reduce the dimensions of this subset of our data from $3$ to $1$ dimension.\n",
        "\n",
        "We'll start by looking at the hand data using $3$ features because $3$ is something we can plot using $3D$ graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ANSUR_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/ansur.json\"\n",
        "ansur_data = object_from_json_url(ANSUR_FILE)\n",
        "\n",
        "ansur_df = pd.json_normalize(ansur_data)\n",
        "display(ansur_df.head())\n",
        "\n",
        "hand_df = ansur_df[[\"hand.breadth\", \"hand.length\", \"hand.palm\"]]\n",
        "\n",
        "# scale and pca objects\n",
        "hand_mss = StandardScaler()\n",
        "hand_pca = PCA(n_components=1)\n",
        "\n",
        "# perform scaling and pca\n",
        "hand_df = hand_mss.fit_transform(hand_df)\n",
        "hand_pca_df = hand_pca.fit_transform(hand_df)\n",
        "\n",
        "# project from 1 PCA back to original 3 dimensions\n",
        "hand_pca_i_df = hand_pca.inverse_transform(hand_pca_df)\n",
        "\n",
        "# plot\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "# Original 3D data in x,y,z space\n",
        "ax.scatter(hand_df[\"hand.breadth\"], hand_df[\"hand.length\"], hand_df[\"hand.palm\"], s=3, alpha=0.15)\n",
        "\n",
        "# Original data projected onto each of the 3 axis\n",
        "ax.scatter(hand_df[\"hand.breadth\"], [-3.5]*len(hand_df[\"hand.length\"]), [-3.5]*len(hand_df[\"hand.palm\"]), s=3, c='#ff8000', alpha=0.333)\n",
        "ax.scatter([-3.5]*len(hand_df[\"hand.breadth\"]), hand_df[\"hand.length\"], [-3.5]*len(hand_df[\"hand.palm\"]), s=3, c='#ff8000', alpha=0.333)\n",
        "ax.scatter([-3.5]*len(hand_df[\"hand.breadth\"]), [-3.5]*len(ansur_df[\"hand.length\"]), hand_df[\"hand.palm\"], s=3, c='#ff8000', alpha=0.333)\n",
        "\n",
        "# Data projected onto the first principal component\n",
        "ax.scatter(hand_pca_i_df[\"hand.breadth\"], hand_pca_i_df[\"hand.length\"], hand_pca_i_df[\"hand.palm\"], s=3, c='r', alpha=0.333)\n",
        "\n",
        "# Axis limits\n",
        "ax.set_xlim((-4, 4))\n",
        "ax.set_ylim((-4, 4))\n",
        "ax.set_zlim((-4, 4))\n",
        "\n",
        "# Axis Labels\n",
        "ax.set_xlabel(\"Hand Breadth\")\n",
        "ax.set_ylabel(\"Hand Length\")\n",
        "ax.set_zlabel(\"Palm Length\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph above shows the original data for the `hand.breadth`, `hand.length` and `hand.palm` features in blue.\n",
        "\n",
        "The orange points show the data projected onto the different axis. It is $1$ dimensional, meaning that in each of those cases we've completely ignored two features.\n",
        "\n",
        "The red points show our data projected onto the first principal component.\n",
        "\n",
        "If we call the `pca.explained_variance()` function we can see how much of our data's information has been retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hand_pca.explained_variance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA in all dimensions\n",
        "\n",
        "What if we reduce all of our features to just $1$? How much of the data's information is kept?\n",
        "\n",
        "If we want to keep $80\\%$ of our data's variance, how many components should our PCA use?\n",
        "\n",
        "No need to plot anything, just run pca a few times and look at its `explained_variance`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Repeat above process, using all of the original features, except gender\n",
        "mss = StandardScaler()\n",
        "ansur_df = pd.json_normalize(ansur_data).drop(columns=[\"gender\"])\n",
        "ansur_df = mss.fit_transform(ansur_df)\n",
        "\n",
        "## TODO: get explained_variance() for n_components=1\n",
        "ansur_pca = PCA(n_components=1)\n",
        "ansur_pca_df = ansur_pca.fit_transform(ansur_df)\n",
        "print(\"n: 1, variance:\", ansur_pca.explained_variance())\n",
        "\n",
        "\n",
        "## TODO: find n_components for explained_variance >= 0.80\n",
        "ansur_pca = PCA(n_components=5)\n",
        "ansur_pca_df = ansur_pca.fit_transform(ansur_df)\n",
        "print(\"n: 4, variance:\", ansur_pca.explained_variance())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "It takes more than $3$ components to keep $80\\%$ of the information from our data. This isn't something we can easily plot to compare to the distribution of the original $14$ features.\n",
        "\n",
        "We can plot the first $2$ and first $3$ principle components, but after that we just have to trust the `explained_variance` number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### More PCA in Multiple Dimensions\n",
        "\n",
        "The above exercises are good for us to start building intuition about PCA, but even though we can perform PCA and reduce our features from $15$ to $5$ or $6$ principal components while retaining most of the variation in our data, it gets hard to visualize the effects of these transformations.\n",
        "\n",
        "Let's use images. We'll load images with about $10\\text{,}000$ pixels/features, and perform PCA to represent them using only $10$ components. Then we'll remap our $10$ components for each image back to $10\\text{,}000$ pixels to look at the effects of **_compressing_** our data like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA on Images\n",
        "\n",
        "Let's load all of the images inside the `data/imgs/att-faces/` directory. These images are in sub-directories that specify the `id` of the person in the image: `s0` is `subject 0`, `s1` is `subject 1`, and so on.\n",
        "\n",
        "We'll keep track of these ids and create numeric labels for them as we read the data from the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lists for keeping track of image pixel lists, subject numeric id and subject label\n",
        "face_pixels = []\n",
        "face_ids = []\n",
        "id2label = []\n",
        "\n",
        "# 40 directories\n",
        "for l in range(1, 41):\n",
        "  id2label.append(f\"s{l}\")\n",
        "  # 10 images per directory\n",
        "  for i in range(1, 11):\n",
        "    mimg = open_image(f\"./data/imgs/att-faces/s{l}/{i}.pgm\")\n",
        "    face_pixels.append(mimg.pixels)\n",
        "    face_ids.append(l)\n",
        "\n",
        "# display first image\n",
        "display(make_image(face_pixels[0], width=92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print len of lists to make sure sizes match\n",
        "print(len(face_pixels), len(face_ids))\n",
        "\n",
        "# check number of labels matches number of directories\n",
        "print(len(id2label))\n",
        "\n",
        "# check how many pixels per image and look at first 10 pixel values\n",
        "print(len(face_pixels[0]), face_pixels[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These look ok. There's only one color value per pixel since these are grayscale images.\n",
        "\n",
        "#### Run PCA\n",
        "\n",
        "We can run PCA directly on these lists of pixels. We don't even have to scale the data because we know the pixel values are all between $0$ and $255$ for greyscale images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run pca and get first 10 PCs\n",
        "pca = PCA(n_components=10)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "\n",
        "# put id values in DataFrame for classification later\n",
        "faces_df[\"id\"] = face_ids\n",
        "\n",
        "display(faces_df.head())\n",
        "\n",
        "print(pca.explained_variance())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "Our PCA-transformed data retained about $60\\%$ of the information from the original data.\n",
        "\n",
        "Our `DataFrame` now is expressed in terms of the first $10$ principal components of our data. The values for these new features are all over the place, and don't represent pixel colors anymore. They represent our data in a more _abstract_, _latent_, space. It's latent because the information is there, it exists, but it's sort of waiting to be transformed back into physical values and, in this case, visualized as pixels.\n",
        "\n",
        "This `PCA` transformation was able to compress our data and reduce the number of dimensions for each of our images from $10\\text{,}304$ to $10$ ! That's a reduction factor of $1000$ ! We could start thinking about doing clustering or classification by measuring distances between these images because now they only have $10$ dimensions, and each of the dimensions represents a combination of color information, not just one pixel.\n",
        "\n",
        "But, let's visualize our new compressed images first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconstruction: un-projects the data from PCA space into pixel space\n",
        "pca_pixels = pca.inverse_transform(faces_df)\n",
        "\n",
        "display(make_image(face_pixels[0], width=92))\n",
        "display(make_image(pca_pixels.loc[0], width=92))\n",
        "\n",
        "display(make_image(face_pixels[110], width=92))\n",
        "display(make_image(pca_pixels.loc[110], width=92))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🤔\n",
        "\n",
        "We did lose $40\\%$ of the detail of our data when we transformed it into `PCA` space.\n",
        "\n",
        "These don't look too good at first, but they also don't look that bad. Given how regular all of the images are, maybe this level of detail is all that is needed to do any kind of classification or clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### More components\n",
        "\n",
        "Re-run PCA with $20$ and $120$ components. Don't worry about printing the `DataFrame`, but reconstruct some of the images like we did in the cell above.\n",
        "\n",
        "What is the effect of keeping more components in our PCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: reconstruct images using 20 and 120 component PCAs\n",
        "\n",
        "## No need to re-write the code here, we can just change the parameters and re-run the cells above\n",
        "pca = PCA(n_components=20)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "print(pca.explained_variance())\n",
        "\n",
        "pca_pixels = pca.inverse_transform(faces_df)\n",
        "\n",
        "display(make_image(face_pixels[0], width=92))\n",
        "display(make_image(pca_pixels.loc[0], width=92))\n",
        "\n",
        "display(make_image(face_pixels[110], width=92))\n",
        "display(make_image(pca_pixels.loc[110], width=92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: reconstruct images using 20 and 120 component PCAs\n",
        "\n",
        "## No need to re-write the code here, we can just change the parameters and re-run the cells above\n",
        "pca = PCA(n_components=120)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "print(pca.explained_variance())\n",
        "\n",
        "pca_pixels = pca.inverse_transform(faces_df)\n",
        "\n",
        "display(make_image(face_pixels[0], width=92))\n",
        "display(make_image(pca_pixels.loc[0], width=92))\n",
        "\n",
        "display(make_image(face_pixels[110], width=92))\n",
        "display(make_image(pca_pixels.loc[110], width=92))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common features\n",
        "\n",
        "One way to think about how `PCA` compresses our features, is to imagine that it can factor out common elements between features that are strongly correlated. For numerical data, those common factors are just parameters to a series of multiplications. Boring.\n",
        "\n",
        "With images, the common factors between our features can actually be seen because our features represent pixels.\n",
        "\n",
        "We can iterate over the `PCA` factors (or, `components`) and display them as images. We just have to scale them to pixel range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for comp in pca.components_[:5]:\n",
        "  minVal = comp.min()\n",
        "  maxVal = comp.max()\n",
        "  # manually mapping to [0, 1]\n",
        "  pxs01 = (comp - minVal) / (maxVal - minVal)\n",
        "  # mapping to [0, 255]\n",
        "  pxs = [255 * p for p in pxs01]\n",
        "  display(make_image(pxs, width=92))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Having $10$ components in our transformation means having $10$ of these \"common\" faces and $10$ features in our transformed dataset. The original faces are reconstructed (with error) by multiplying the $10$ features in the transformed dataset by their corresponding face, so `PC0` with the first face, `PC1` with the second face, and so on, and then adding those results.\n",
        "\n",
        "These top-5 \"common\" faces will always be the same for this dataset, independent of the number of components we keep in our `PCA` transformation. \n",
        "\n",
        "Adding components to our transformation just increases the number of these \"common\" faces that get extracted, but their order doesn't change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Back to $10$ components\n",
        "\n",
        "Let's go back to just using $10$ PCs and see how the first $100$ images are distributed in the first $2$ dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run pca and get first 10 PCs\n",
        "pca = PCA(n_components=10)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "\n",
        "# put id values in DataFrame\n",
        "faces_df[\"id\"] = face_ids\n",
        "\n",
        "print(pca.explained_variance())\n",
        "\n",
        "# first 10 ids\n",
        "x = faces_df[\"PC0\"][:100]\n",
        "y = faces_df[\"PC1\"][:100]\n",
        "z = faces_df[\"PC2\"][:100]\n",
        "c = faces_df[\"id\"][:100]\n",
        "\n",
        "# 2D\n",
        "plt.scatter(x, y, c=c, marker='o', linestyle='', alpha=1, cmap=\"tab10\")\n",
        "plt.title(\"Principal Components\")\n",
        "plt.xlabel(\"PC 0\")\n",
        "plt.ylabel(\"PC 1\")\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=c, marker='o', linestyle='', alpha=1, cmap=\"tab10\")\n",
        "plt.title(\"Principal Components\")\n",
        "ax.set_xlabel(\"PC 0\")\n",
        "ax.set_ylabel(\"PC 1\")\n",
        "ax.set_ylabel(\"PC 1\")\n",
        "\n",
        "ax.set_xlim((-3000, 3000))\n",
        "ax.set_ylim((-500, 3000))\n",
        "# ax.set_zlim((-4, 4))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "Here we are only looking at the top $2$ and $3$ dimensions of our data and we can already see some patterns/clusters.\n",
        "\n",
        "The colors were added manually using the correct `id` for each image, so that helps to see patterns...\n",
        "\n",
        "Now, let's run the `RandomForestClassifier` algorithm on this dataset to see if PCA dimensionality reduction can actually help prepare image datasets for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA + Classification\n",
        "\n",
        "We have already prepared our `DataFrame` and added a column with the numerical `id` of each image.\n",
        "\n",
        "We can check by running `faces_df.head()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "faces_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Separate Train and Test Data\n",
        "\n",
        "In order to measure how well our classifier works in general, we should separate our data into $2$ subsets, one which will get used to train our model, and another that we can use to measure how well our model performs on data it hasn't seen before.\n",
        "\n",
        "This is to avoid having our model memorize the training data, and then performing horrible on new data in the future.\n",
        "\n",
        "We'll use the `Scikit-Learn` function `train_test_split` to split our `DataFrame` into $2$ equal datasets.\n",
        "\n",
        "Since the split is random, the `while` loop below is just to guarantee that our test dataset has images from all subjects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ids = []\n",
        "\n",
        "while len(set(test_ids)) != len(id2label):\n",
        "  faces_train_df, faces_test_df = train_test_split(faces_df, test_size=0.5)\n",
        "  test_ids = faces_test_df[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(faces_train_df.head())\n",
        "display(faces_test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Classifier\n",
        "\n",
        "The below steps should look familiar.\n",
        "\n",
        "We split the output feature from he rest of the features, and pass those to the `fit()` function of an instance of the `RandomForestClassifier` class.\n",
        "\n",
        "We're not re-scaling our components/features from `PCA`, because we're assuming that any difference in the magnitude of those features is actually important information that the algorithm can use to create a model. And, `RandomForest` and `DecisionTree` classifiers tend to be a little less sensitive to scaling because of their `if`/`else` nature.\n",
        "\n",
        "That's not always the case. Other modeling methods/algorithms might benefit from having standardized data to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_model = RandomForestClassifier()\n",
        "\n",
        "train_features = faces_train_df.drop(columns=[\"id\"])\n",
        "train_ids = faces_train_df[\"id\"]\n",
        "\n",
        "# Create a model that classifies faces based on principal components\n",
        "face_model.fit(train_features, train_ids)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "train_predicted = face_model.predict(train_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(train_ids, train_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Error should be close to $0$.\n",
        "\n",
        "#### Test Data\n",
        "\n",
        "Now we'll run the `predict()` function of our classifier on our test dataset and see how it performs.\n",
        "\n",
        "Again, this should look familiar: we just have to remember to separate the output feature (`id`) from the other features, and then pass the list of independent features to the `predict()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_features = faces_test_df.drop(columns=[\"id\"])\n",
        "test_ids = faces_test_df[\"id\"]\n",
        "\n",
        "## 6. Run the model on the test data\n",
        "test_predicted = face_model.predict(test_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(test_ids, test_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "The Classification Error should be close to $20\\%$.\n",
        "\n",
        "This isn't bad !\n",
        "\n",
        "Remember, we just loaded up some unknown dataset of images, automatically reduced the number of dimensions in each image from $10\\text{,}304$ to $10$ and used a basic `RandomForestClassifier()` to learn patterns on the compressed data.\n",
        "\n",
        "We didn't have to look at pixels, we didn't have to manually filter images, we didn't even have to guess which features to keep. PCA helped with all of this.\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "We can run the following cell to print a **_Confusion Matrix_** for our model. This is a graph that shows how well our model performed on each class of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_confusion_matrix(test_ids, test_predicted, display_labels=id2label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's probably a class or two that our model didn't do too well with.\n",
        "\n",
        "#### Improving Our Model\n",
        "\n",
        "How can we improve this model ? We picked $10$ components just to see what would happens with a dimensionality reduction of $1000$.\n",
        "\n",
        "What happens if we use $20$ components? $20$ is still a lot less than $10\\text{,}304$ and probably still easy for our `RandomForestClassifier` to handle.\n",
        "\n",
        "How much of the data's variance is kept with $20$ components?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: Re-run the classification training, but using 16, 24, 32 PCs\n",
        "\n",
        "# run pca and get first 16,24,32 PCs\n",
        "pca = PCA(n_components=16)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "\n",
        "# put id values in DataFrame\n",
        "faces_df[\"id\"] = face_ids\n",
        "\n",
        "print(pca.explained_variance())\n",
        "display(faces_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ids = []\n",
        "\n",
        "while len(set(test_ids)) != len(id2label):\n",
        "  faces_train_df, faces_test_df = train_test_split(faces_df, test_size=0.5)\n",
        "  test_ids = faces_test_df[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_features = faces_train_df.drop(columns=[\"id\"])\n",
        "train_ids = faces_train_df[\"id\"]\n",
        "\n",
        "test_features = faces_test_df.drop(columns=[\"id\"])\n",
        "test_ids = faces_test_df[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_model = RandomForestClassifier()\n",
        "\n",
        "# Create a model that classifies faces based on principal components\n",
        "face_model.fit(train_features, train_ids)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "train_predicted = face_model.predict(train_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(train_ids, train_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Run the model on the test data\n",
        "test_predicted = face_model.predict(test_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(test_ids, test_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation \n",
        "\n",
        "In order to improve the classifier it might not be enough to just increase the number of components. It could be that this is as good as a `RandomForestClassifier` will do when given $10$ to $32$ Principal Component features.\n",
        "\n",
        "Using all possible PC features ($400$), on the other hand, definitely makes our model worse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "Unsupervised learning can be a very powerful and useful tool for performing exploratory data analysis and data pre-processing.\n",
        "\n",
        "Because there's no labeled/correct answer in unsupervised learning, we can be a bit more subjective in how we pick our metrics for success."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
